#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Oct  2 12:15:57 2023

@author: mkcc68


Dataset:
    (image, attributes, identity)
Generator:
    input = input Image or nothing
    Output = Image that looks similar to input image
Classifier:
    input: Image
    Output:
        input image -> correct attr
        protected image -> incorrect attr
Identifier: 
    Input: Image
    Output: (correct) identity
    Probably frozen.
    
"""

from torch import nn
import torch
from itertools import chain
from torchvision.models  import vgg16
class AttrHider():
    
    @staticmethod
    def Encoder():
        return nn.Sequential(
                nn.Conv2d(3, 64, 4, 2, padding = 1 ),
                nn.InstanceNorm2d(64),
                nn.LeakyReLU(),
                nn.Conv2d(64, 128, 4, 2, padding = 1),
                nn.InstanceNorm2d(128),
                nn.LeakyReLU(),
                nn.Conv2d(128, 256, 4, 2, padding = 1),
                nn.InstanceNorm2d(256),
                nn.LeakyReLU(),
                nn.Conv2d(256, 512, 4, 2, padding = 1),
                nn.InstanceNorm2d(512),
                nn.LeakyReLU(),
                nn.Conv2d(512, 1024, 4, 2, padding = 1),
                nn.InstanceNorm2d(1024),
                nn.LeakyReLU(),
            )
    
    class Generator(nn.Module):
        def __init__(self):
            super().__init__()
            self.encoder_layers = nn.ModuleList()
            self.decoder_layers = nn.ModuleList()
            channels = [3, 64, 128, 256, 512, 1024]
            for i in range(5):
                
                self.encoder_layers.append(nn.Sequential(
                        nn.Conv2d(channels[i], channels[i+1], 4, 2, padding = 1),
                        nn.InstanceNorm2d(channels[i+1]),
                        nn.LeakyReLU()
                    ))
                self.decoder_layers.append(nn.Sequential(
                        nn.ConvTranspose2d(channels[-i-1],channels[-i-2], 4, 2, padding = 1),
                        nn.InstanceNorm2d(channels[-i-2]),
                        nn.LeakyReLU()
                    ))
            
            
        def forward(self, x):
            for l in self.encoder_layers:
                x = l(x)
            x = torch.rand(len(x),1024,4,4) - x
            for l in self.decoder_layers:
                x = l(x)
            return x
    
    def __init__(self):
        self.classifier = self.Encoder()
        self.classifier.add_module('maxpool',nn.MaxPool2d(4))
        self.classifier.add_module('linear', nn.Linear(1024,40))
        self.detector = self.Encoder()
        self.detector.add_module('maxpool',nn.MaxPool2d(4))
        self.detector.add_module('detector', nn.Linear(1024,1))
        self.generator = self.Generator()
        self.identifier = vgg16()
        self.optimizer_D = torch.optim.Adam(chain(self.detector.parameters(),self.classifier.parameters()),
                                            lr = 0.0002, betas=(0.5,0.99) )
        self.optimizer_G = torch.optim.Adam(self.generator.parameters(), lr = 0.0002,betas=(0.5,0.99))
        
    
    def train_step(self,n=5):
        
        def calc_gradient(network, orig_images, protected_images):
            epsilon = torch.rand(self.bach_size)
            merged_images = epsilon * orig_images + (1-epsilon) * protected_images
            merged_images.require_grad = True
            fun(merged_images).backward()
            return merged_images.grad
        
        for t in range(n):
            orig_images, classifications = self.dataset.get_dset(attr = True)
            
            protected_images = self.generator(orig_images)
            images_attr = self.classifier(torch.concat((orig_images[2:], protected_images[2:]))) 
            classifications = torch.concat((classifications[2:], classifications[2:]))
            
            grad_x = calc_gradient(self.detector)
            gradient_penalty = 10 * torch.square(torch.norm(grad_x) - 1)               
            
            
            loss = self.detector(protected_images) - self.detector(orig_images) 
            loss += nn.functional.binary_cross_entropy_with_logits(images_attr, classifications) 
            loss += gradient_penalty
            
            self.detector.requires_grad_ = True
            self.classifier.requires_grad_ = True
            
            loss.backward()
            self.optimizer_D.zero_grad()
            self.optimizer_D.step()
            
            self.classifier.requires_grad_ = False
            self.detector.requires_grad_ = False
            
        orig_images, classifications, identifications = self.dataset.get_dset(attr = True, identity = True)
        
        protected_images = self.generator(orig_images)
        protected_images_attr = self.classifier(protected_images)
        protected_images_id = self.identifier(protected_images)
        loss = -self.detector(self.generator)
        loss += -nn.functional.binary_cross_entropy_with_logits(protected_images_attr, classifications)
        loss += nn.functional.binary_cross_entropy_with_logits(protected_images_id, identifications)
        
        self.generator.requires_grad_ = True
        
        loss.backward()
        self.optimizer_G.zero_grad()
        self.optimizer_G.step()
        loss.forward()
        
        
        